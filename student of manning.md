## Sepandar Kamvar
在搞一些奇怪的东西
## Lynn Berry
同上
## Dan Klein
https://people.eecs.berkeley.edu/~klein/
My education, in reverse order:
	Stanford University 	MS, PhD in Computer Science 	1999-2004
	Oxford University, St. John's College 	MSt in Linguistics 	1998-1999
	Cornell University 	BA in Math, CS, Linguistics (summa cum laude) 	1994-1998
	Mt. Lebanon High School 	  	1990-1994

Some fellowships / awards:

    Diane S. McEntyre Award for Excellence in Teaching, 2011
    UC Berkeley Distinguished Teaching Award, 2010
    Jim and Donna Gray Award for Excellence in UG Teaching, 2009
    Okawa Research Award, 2009
    ACM Grace Murray Hopper Award, 2007
    Alfred P. Sloan Fellowship, 2007
    NSF CAREER Award, 2007
    Microsoft Faculty Fellowship, 2005
    Microsoft Graduate Fellowship, 2003
    British Marshall Fellowship, 1998

### EMNLP Analogs of Linguistic Structure in Deep Representations
Abstract
We  investigate  the  compositional  struc-
ture  of  message  vectors  computed  by  a
deep network trained on a communication
game.
By  comparing  truth-conditional
representations of encoder-produced mes-
sage  vectors  to  human-produced  refer-
ring  expressions,  we  are  able  to  identify
aligned  (vector,  utterance)  pairs  with  the
same meaning.  We then search for struc-
tured  relationships  among  these  aligned
pairs   to   discover   simple   vector   space
transformations   corresponding   to   nega-
tion,  conjunction,  and  disjunction.    Our
results suggest that neural representations
are  capable  of  spontaneously  developing
a  “syntax”  with  functional  analogues  to
qualitative properties of natural language

### EMNLP Effective Inference for Generative Neural Parsing
Abstract
Generative  neural  models  have  recently
achieved  state-of-the-art  results  for  con-
stituency parsing. However, without a fea-
sible search procedure, their use has so far
been limited to reranking the output of ex-
ternal parsers in which decoding is more
tractable.    We  describe  an  alternative  to
the conventional action-level beam search
used for discriminative neural models that
enables us to decode directly in these gen-
erative models.  We then show that by im-
proving our basic candidate selection strat-
egy and using a coarse pruning function,
we  can  improve  accuracy  while  explor-
ing significantly less of the search space.
Applied to the model of Choe and Char-
niak (2016),  our inference procedure ob-
tains 92.56 F1 on section 23 of the Penn
Treebank, surpassing prior state-of-the-art
results for single-model systems.

### EMNLP Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space
Abstract
We  present  a  model  for  locating  regions
in  space  based  on  natural  language  de-
scriptions.   Starting with a 3D scene and
a sentence, our model is able to associate
words in the sentence with regions in the
scene,  interpret  relations  such  as on  top
of or next  to,and  finally  locate  the  re-
gion described in the sentence.  All com-
ponents form a single neural network that
is trained end-to-end without prior knowl-
edge  of  object  segmentation.    To  evalu-
ate our model, we construct and release a
new dataset consisting of Minecraft scenes
with  crowdsourced  natural  language  de-
scriptions.  We achieve a 32% relative er-
ror reduction compared to a strong neural
baseline.

### ACL A Minimal Span-Based Neural Constituency Parser
Abstract
In this work, we present a minimal neural
model  for  constituency  parsing  based  on
independent  scoring  of  labels  and  spans.
We  show  that  this  model  is  not  only
compatible  with  classical  dynamic  pro-
gramming  techniques,  but  also  admits  a
novel  greedy  top-down  inference  algo-
rithm  based  on  recursive  partitioning  of
the  input.
We  demonstrate  empirically
that both prediction schemes are competi-
tive with recent work, and when combined
with basic extensions to the scoring model
are  capable  of  achieving  state-of-the-art
single-model  performance  on  the  Penn
Treebank  (91.79  F1)  and  strong  perfor-
mance on the French Treebank (82.23 F1).

### ACL Abstract Syntax Networks for Code Generation and Semantic Parsing
Abstract
Tasks  like  code  generation  and  semantic
parsing require mapping unstructured (or
partially structured) inputs to well-formed,
executable  outputs.
We  introduce  abstract syntax networks, a modeling framework  for  these  problems.
The  outputs are  represented  as  abstract  syntax  trees (ASTs) and constructed by a decoder with
a dynamically-determined modular struc-
ture paralleling the structure of the output
tree.   On  the benchmark  HEARTHSTONE
dataset for code generation, our model ob-
tains 79.2 BLEU and 22.7% exact match
accuracy,  compared  to  previous  state-of-
the-art values of 67.1 and 6.1%.  Further-
more,  we  perform  competitively  on  the
ATIS,  JOBS,  and  GEO semantic  parsing
datasets with no task-specific engineering.

### ACL Fine-Grained Entity Typing with High-Multiplicity Assignments
Abstract
As entity type systems become richer and
more fine-grained, we expect the number
of types assigned to a given entity to in-
crease. However, most fine-grained typing
work has focused on datasets that exhibit
a low degree of type multiplicity.  In this
paper,  we  consider  the  high-multiplicity
regime  inherent  in  data  sources  such  as
Wikipedia  that  have  semi-open  type  sys-
tems.   We  introduce  a  set-prediction  ap-
proach to this problem and show that our
model outperforms unstructured baselines
on  a  new  Wikipedia-based  fine-grained
typing corpus.

### ACL Improving Neural Parsing by Disentangling Model Combination and Reranking Effects
Abstract
Recent work has proposed several genera-
tive neural models for constituency pars-
ing  that  achieve  state-of-the-art  results.
Since  direct  search  in  these  generative
models  is  difficult,  they  have  primarily
been  used  to  rescore  candidate  outputs
from  base  parsers  in  which  decoding  is
more straightforward.  We first present an
algorithm  for  direct  search  in  these  gen-
erative models.  We then demonstrate that
the rescoring results are at least partly due
to implicit model combination rather than
reranking  effects.   Finally,  we  show  that
explicit  model  combination  can  improve
performance even further, resulting in new
state-of-the-art  numbers  on  the  PTB  of
94.25 F1 when training only on gold data
and 94.66 F1 when using external data.

### ACL Translating Neuralese
Abstract
Several approaches have recently been pro-
posed for learning decentralized deep mul-
tiagent policies that coordinate via a dif-
ferentiable communication channel. While
these policies are effective for many tasks,
interpretation of their induced communi-
cation strategies has remained a challenge.
Here we propose to interpret agents’ mes-
sages by translating them. Unlike in typi-
cal machine translation problems, we have
no parallel data to learn from. Instead we
develop a translation model based on the
insight that agent messages and natural lan-
guage strings mean the same thing
if they
induce the same belief about the world in a
listener
. We present theoretical guarantees
and empirical evidence that our approach
preserves both the semantics and pragmat-
ics of messages by ensuring that players
communicating through a translation layer
do not suffer a substantial loss in reward rel-
ative to players with a common language.

### ICML Modular Multitask Reinforcement Learning with Policy Sketches
Abstract
We describe a framework for multitask deep re-
inforcement learning guided by
policy sketches
.
Sketches annotate tasks with sequences of named
subtasks, providing information about high-level
structural relationships among tasks but not how
to  implement  them—specifically  not  providing
the  detailed  guidance  used  by  much  previous
work on learning policy abstractions for RL (e.g.
intermediate  rewards,  subtask  completion  sig-
nals,  or  intrinsic  motivations).    To  learn  from
sketches, we present a model that associates ev-
ery subtask with a modular subpolicy, and jointly
maximizes  reward  over  full  task-specific  poli-
cies by tying parameters across shared subpoli-
cies.  Optimization is accomplished via a decou-
pled actor–critic training objective that facilitates
learning  common  behaviors  from  multiple  dis-
similar reward functions.  We evaluate the effec-
tiveness  of  our  approach  in  three  environments
featuring  both  discrete  and  continuous  control,
and with sparse rewards that can be obtained only
after  completing  a  number  of  high-level  sub-
goals. Experiments show that using our approach
to learn policies guided by sketches gives better
performance than existing techniques for learn-
ing  task-specific  or  shared  policies,  while  nat-
urally inducing a library of interpretable primi-
tive behaviors that can be recombined to rapidly
adapt to new tasks.

### ACL Parsing with Traces: An O(n^4) Algorithm and a Structural Representation
Abstract
General  treebank  analyses  are  graph  struc-
tured,  but  parsers  are  typically  restricted  to
tree structures for efficiency and modeling rea-
sons.   We  propose  a  new  representation  and
algorithm for a class of graph structures that
is flexible enough to cover almost all treebank
structures, while still admitting efficient learn-
ing and inference.  In particular, we consider
directed, acyclic, one-endpoint-crossing graph
structures,   which  cover  most  long-distance
dislocation, shared argumentation, and similar
tree-violating  linguistic  phenomena.   We  de-
scribe how to convert phrase structure parses,
including traces, to our new representation in
a  reversible  manner.   Our  dynamic  program
uniquely decomposes structures, is sound and
complete, and covers 97.3%of the Penn En-
glish Treebank.  We also implement a proof-
of-concept parser that recovers a range of null
elements and trace types.

### EMNLP Reasoning About Pragmatics with Neural Listeners and Speakers
Abstract
We present a model for contrastively describ-
ing  scenes,  in  which  context-specific  behav-
ior  results  from  a  combination  of  inference-
driven pragmatics and learned semantics. Like
previous learned approaches to language gen-
eration,   our  model  uses  a  simple  feature-
driven architecture (here a pair of neural “lis-
tener” and “speaker” models) to ground lan-
guage in the world.  Like inference-driven ap-
proaches  to  pragmatics,  our  model  actively
reasons  about  listener  behavior  when  select-
ing utterances.  For training, our approach re-
quires only ordinary captions, annotated
without demonstration of the pragmatic behavior
the model ultimately exhibits. In human eval-
uations  on  a  referring  expression  game,  our
approach succeeds 81% of the time, compared
to 69% using existing techniques.

### ACL Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints
Abstract
We  present  a  discriminative  model  for
single-document summarization that
integrally    combines    compression    and
anaphoricity   constraints.
Our   model selects   textual   units   to   include   in   the summary  based  on  a  rich  set  of  sparse
features  whose  weights  are  learned  on  a
large  corpus.   We  allow  for  the  deletion
of  content  within  a  sentence  when  that
deletion is licensed by compression rules;
in our framework, these are implemented
as   dependencies   between   subsentential
units  of  text.
Anaphoricity  constraints then   improve   cross-sentence   coherence by  guaranteeing  that,  for  each  pronoun included  in  the  summary,  the  pronoun’s antecedent   is   included   as   well   or   the pronoun  is  rewritten  as  a  full  mention.
When  trained  end-to-end,  our  final  system
1 outperforms   prior   work   on   both
ROUGE as well as on human judgments
of linguistic quality.

### NAACL Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks
Abstract
A key challenge in entity linking is making ef-
fective  use  of  contextual  information  to  dis-
ambiguate mentions that might refer to differ-
ent entities in different contexts.  We present
a  model  that  uses  convolutional  neural  net-
works to capture semantic correspondence be-
tween a mention’s context and a proposed tar-
get entity. These convolutional networks oper-
ate at multiple granularities to exploit various
kinds of topic information, and their rich pa-
rameterization gives them the capacity to learn
which n-grams  characterize  different  topics.
We combine these networks with a sparse lin-
ear  model  to  achieve  state-of-the-art  perfor-
mance on multiple entity linking datasets, out-
performing  the  prior  systems  of  Durrett  and
Klein (2014) and Nguyen et al. (2014).

### NAACL Learning to Compose Neural Networks for Question Answering
Abstract
We describe a question answering model that
applies to both images and structured knowl-
edge  bases. The  model  uses  natural  lan-
guage strings to automatically assemble neu-
ral networks from a collection of composable
modules.   Parameters  for  these  modules  are
learned jointly with network-assembly param-
eters  via  reinforcement  learning,  with  only
(world,  question,  answer)  triples  as  supervi-
sion. Our approach, which we term a
dynamic neural module network
, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.

### CVPR Neural Module Networks
Abstract
Visual  question  answering  is  fundamentally  compositional in nature—a question like
where is the dog?
shares substructure with questions like
what color is the dog?
and where is the cat?
This paper seeks to simultaneously exploit
the representational capacity of deep networks and the compositional linguistic structure of questions.  We describe a procedure for constructing and learning neural module net-
works, which compose collections of jointly-trained neural“modules” into deep networks for question answering. Our approach  decomposes  questions  into  their  linguistic  substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.).  The resulting compound  networks  are  jointly  trained.   We  evaluate  our  approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex ques-
tions about abstract shapes.

## Kristina Toutanova
在microsoft
### ACL A Nested Attention Neural Hybrid Model for Grammatical Error Correction
Abstract
Grammatical error correction (GEC) sys-
tems strive to correct both global errors in
word order and usage, and local errors in
spelling and inflection.  Further develop-
ing upon recent work on neural machine
translation, we propose a new hybrid neural
model with nested attention layers for GEC.
Experiments show that the new model can
effectively correct errors of both types by
incorporating word and character-level in-
formation, and that the model significantly
outperforms  previous  neural  models  for
GEC as measured on the standard CoNLL-
14  benchmark  dataset.   Further  analysis
also shows that the superiority of the pro-
posed model can be largely attributed to
the use of the nested attention mechanism,
which has proven particularly effective in
correcting local errors that involve small
edits in orthography.

### ACL NLP for Precision Medicine
Abstract
Past work in relation extraction has focused
on binary relations in single sentences.   Re-
cent NLP inroads in high-value domains have
sparked  interest  in  the  more  general  setting
of  extracting n-ary  relations  that  span  mul-
tiple sentences.   In this paper,  we explore a
general relation extraction framework based
on graph long short-term memory networks
(graph LSTMs) that can be easily extended to
cross-sentence n-ary relation extraction. The
graph formulation provides a unified way of
exploring different LSTM approaches and in-
corporating various intra-sentential and inter-
sentential  dependencies,  such  as  sequential,
syntactic,  and discourse relations.   A robust
contextual representation is learned for the en-
tities, which serves as input to the relation clas-
sifier. This simplifies handling of relations with
arbitrary arity, and enables multi-task learning
with related relations. We evaluate this frame-
work in two important precision medicine set-
tings, demonstrating its effectiveness with both
conventional supervised learning and distant
supervision.   Cross-sentence  extraction  pro-
duced larger knowledge bases. and multi-task
learning significantly improved extraction ac-
curacy. A thorough analysis of various LSTM
approaches yielded useful insight the impact
of linguistic analysis on extraction accuracy.

### ACL Compositional Learning of Embeddings for Relation Paths in Knowledge Base and Text
Abstract

Modeling relation paths has offered significant gains in embedding models for knowledge base (KB) completion. However, enumerating paths between two entities is very expensive, and existing approaches typically resort to approximation with a sampled subset. This problem is particularly acute when text is jointly modeled with KB relations and used to provide direct evidence for facts mentioned in it. In this paper, we propose the first exact dynamic programming algorithm which enables efficient incorporation of all relation paths of bounded length, while modeling both relation types and intermediate nodes in the compositional path representations. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion.

### LREC E-TIPSY: Search Query Corpus Annotated with Entities, Term Importance, POS Tags, and Syntactic Parses
Abstract

We present E-TIPSY, a search query corpus annotated with named Entities, Term Importance, POS tags, and SYntactic parses. This corpus contains crowdsourced (gold) annotations of the three most important terms in each query. In addition, it contains automatically produced annotations of named entities, part-of-speech tags, and syntactic parses for the same queries. This corpus comes in two formats: (1) Sober Subset: annotations that two or more crowd workers agreed upon, and (2) Full Glass: all annotations. We analyze the strikingly low correlation between term importance and syntactic headedness, which invites research into effective ways of combining these different signals. Our corpus can serve as a benchmark for term importance methods aimed at improving search engine quality and as an initial step toward developing a dataset of gold linguistic analysis of web search queries. In addition, it can be used as a basis for linguistic inquiries into the kind of expressions used in search.

### EMNLP A Dataset and Evaluation Metrics for Abstractive Compression of Sentences and Short Paragraphs

Abstract
We   introduce   a   manually-created,    multi-
reference dataset for abstractive sentence and
short paragraph compression. First, we exam-
ine  the  impact  of  single-  and  multi-sentence
level  editing  operations  on  human  compres-
sion quality as found in this corpus.  We ob-
serve  that  substitution  and  rephrasing  opera-
tions are more meaning preserving than other
operations,  and  that  compressing  in  context
improves quality.  Second, we systematically
explore  the  correlations  between  automatic
evaluation  metrics  and  human  judgments  of
meaning  preservation  and  grammaticality  in
the compression task, and analyze the impact
of the linguistic units used and precision ver-
sus recall measures on the quality of the met-
rics.   Multi-reference  evaluation  metrics  are
shown to offer significant advantage over sin-
gle reference-based metrics.

## Roger Levy
cv http://www.mit.edu/~rplevy/roger-cv.pdf
好多脑与认知的东西
不是纯nlp
### Modeling Sources of Uncertainty in Spoken Word Learning.  Proceedings of the 39th Annual Meeting of the Cognitive Science Society. 
Abstract
In order to successfully learn the meaning of novel words such as bin or pin, language learners must not only be able to perceive relevant differences in the speech signal but also learn mappings from words to referents. Prior work in native (Stager
& Werker, 1997) and second (Pajak, Creel, & Levy, 2016) language acquisition has found that the ability to perceptually discriminate between words does not guarantee successful word learning. Put differently, learners fail to utilize knowledge that
they can otherwise use in speech perception. To explore possible mechanisms accounting for this phenomenon, we present a probabilistic model capable of inferring a word’s phonetic
form and of integrating the result with prior label/referent representations  in  memory.   By  analyzing  the  reasons  for  successes and failures when fitting different versions of the model
to experimental results from Pajak et al. (2016), we argue that a mechanism for spoken word learning needs to incorporate both perceptual uncertainty as well as additional, task-related sources of uncertainty to successfully account for the data.

### EACL Noisy-context surprisal as a human sentence processing cost model
Abstract
We  use  the  noisy-channel  theory  of  hu-
man  sentence  comprehension  to  develop
an   incremental   processing   cost   model
that   unifies   and   extends   key   features
of  expectation-based  and  memory-based
models. In  this  model,  which  we  call
noisy-context surprisal,  the  processing
cost  of  a  word  is  the  surprisal  of  the
word   given   a   noisy   representation   of
the  preceding  context. We  show  that
this   model   accounts   for   an   outstand-
ing  puzzle  in  sentence  comprehension,
language-dependent  structural  forgetting
effects  (Gibson  and  Thomas,  1999;  Va-
sishth  et  al.,  2010;  Frank  et  al.,  2016),
which are previously not well modeled by
either expectation-based or memory-based
approaches.   Additionally,  we  show  that
this  model  derives  and  generalizes  local-
ity  effects  (Gibson,  1998;  Demberg  and
Keller,  2008),  a  signature  prediction  of
memory-based  models.   We  give  corpus-
based  evidence  for  a  key  assumption  in
this derivation.

## Bill MacCartney
不是很活跃 
最近一篇是14年的
做nlp与stock的
### LREC On the Importance of Text Analysis for Stock Price Prediction
Abstract
We investigate the importance of text analysis for stock price prediction. In particular, we introduce a system that forecasts companies’
stock price changes (UP, DOWN, STAY) in response to financial events reported in 8-K documents. Our results indicate that using text
boosts prediction accuracy over
10%
(relative) over a strong baseline that incorporates many financially-rooted features. This impact is
most important in the short term (i.e., the next day after the financial event) but persists for up to five days.

##  Pi-Chuan Chang
在google工作
最后一篇是10年的

## Jenny Finkel
Engineering Manager + Tech Lead, Machine Learning Team

## Daniel Ramage


As of December 2011, I am a research scientist at Google in Mountain View. My PhD was advised by Christopher D. Manning in the Stanford Natural Language Processing Group, part of the Stanford Artificial Intelligence Lab.

Much of my dissertation work was with the interdisciplinary Mimir Project, focusing on building models and tools that can help us understand the world through the lens of the text people write about it. I develop techniques to analyze academic publications, cluster tagged web pages, and mine Twitter posts, among other applications.

My undergraduate degrees from MIT are in Computer Science and Math. Prior to Stanford, I spent some time at IBM's Zurich Research Lab working on data mining. In Fall 2009, I worked at Microsoft Research in Redmond with Susan Dumais. In summers past, I taught software engineering and project management to Palestinian and Israeli teenagers in Jerusalem through MEET.

### Communication-Efficient Learning of Deep Networks from Decentralized Data
不在搞nlp了好像

## Robert Munro
  
VP of Machine Learning at CrowdFlower Inc.
也不在学术界了

## Marie-Catherine de Marneffe
**I am an assistant professor in Linguistics at The Ohio State University.**

 Stanford University
2006 - 2012
Ph.D. Program in Linguistics

2004 - 2006
Visiting researcher (NLP Group, working with Christopher D. Manning)

Université catholique de Louvain (UCL), Belgium
2000 - 2002
Advanced Studies in Applied Sciences, Computing Science.
Thesis: (Magna cum laude)
Modalités temporelles et aléthiques pour la traduction d'un fragment de la langue française
(Temporal and mandatory modalities for translating a subset of French).

最近只有这一篇
### LREC Universal Stanford dependencies: A cross-linguistic typology
Abstract

Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.


